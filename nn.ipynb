{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fqgp2ns8h9S4"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"nn.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SimpleNetwork:\n",
        "    \"\"\"A simple feedforward network where all units have sigmoid activation.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def random(cls, *layer_units: int):\n",
        "        \"\"\"Creates a feedforward neural network with the given number of units\n",
        "        for each layer.\n",
        "\n",
        "        :param layer_units: Number of units for each layer\n",
        "        :return: the neural network\n",
        "        \"\"\"\n",
        "\n",
        "        def uniform(n_in, n_out):\n",
        "            epsilon = math.sqrt(6) / math.sqrt(n_in + n_out)\n",
        "            return np.random.uniform(-epsilon, +epsilon, size=(n_in, n_out))\n",
        "\n",
        "        pairs = zip(layer_units, layer_units[1:])\n",
        "        return cls(*[uniform(i, o) for i, o in pairs])\n",
        "\n",
        "    def __init__(self, *layer_weights: np.ndarray):\n",
        "        \"\"\"Creates a neural network from a list of weight matrices.\n",
        "        The weights correspond to transformations from one layer to the next, so\n",
        "        the number of layers is equal to one more than the number of weight\n",
        "        matrices.\n",
        "\n",
        "        :param layer_weights: A list of weight matrices\n",
        "        \"\"\"\n",
        "        self.layer_weights = layer_weights\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Sigmoid activation function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_prime(self, z):\n",
        "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "        sig = self.sigmoid(z)\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "    def predict(self, input_matrix: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Performs forward propagation over the neural network starting with\n",
        "        the given input matrix.\n",
        "\n",
        "        Each unit's output should be calculated by taking a weighted sum of its\n",
        "        inputs (using the appropriate weight matrix) and passing the result of\n",
        "        that sum through a logistic sigmoid activation function.\n",
        "\n",
        "        :param input_matrix: The matrix of inputs to the network, where each\n",
        "        row in the matrix represents an instance for which the neural network\n",
        "        should make a prediction\n",
        "        :return: A matrix of predictions, where each row is the predicted\n",
        "        outputs - each in the range (0, 1) - for the corresponding row in the\n",
        "        input matrix.\n",
        "        \"\"\"\n",
        "        activations = input_matrix\n",
        "        for weights in self.layer_weights:\n",
        "            activations = self.sigmoid(np.dot(activations, weights))\n",
        "        return activations\n",
        "\n",
        "    def predict_zero_one(self, input_matrix: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Performs forward propagation over the neural network starting with\n",
        "        the given input matrix, and converts the outputs to binary (0 or 1).\n",
        "\n",
        "        Outputs will be converted to 0 if they are less than 0.5, and converted\n",
        "        to 1 otherwise.\n",
        "\n",
        "        :param input_matrix: The matrix of inputs to the network, where each\n",
        "        row in the matrix represents an instance for which the neural network\n",
        "        should make a prediction\n",
        "        :return: A matrix of predictions, where each row is the predicted\n",
        "        outputs - each either 0 or 1 - for the corresponding row in the input\n",
        "        matrix.\n",
        "        \"\"\"\n",
        "        predictions = self.predict(input_matrix)\n",
        "        return (predictions >= 0.5).astype(int)\n",
        "\n",
        "    def gradients(self,\n",
        "                  input_matrix: np.ndarray,\n",
        "                  output_matrix: np.ndarray) -> List[np.ndarray]:\n",
        "        \"\"\"Performs back-propagation to calculate the gradients for each of\n",
        "        the weight matrices.\n",
        "\n",
        "        This method first performs a pass of forward propagation through the\n",
        "        network, then applies the following procedure to calculate the\n",
        "        gradients.\n",
        "\n",
        "        :param input_matrix: The matrix of inputs to the network, where each\n",
        "        row in the matrix represents an instance for which the neural network\n",
        "        should make a prediction\n",
        "        :param output_matrix: A matrix of expected outputs, where each row is\n",
        "        the expected outputs - each either 0 or 1 - for the corresponding row in\n",
        "        the input matrix.\n",
        "        :return: the gradient matrix for each weight matrix\n",
        "        \"\"\"\n",
        "        # Forward pass\n",
        "        activations = [input_matrix]\n",
        "        zs = []  # List to store all the weighted sums, layer by layer\n",
        "        activation = input_matrix\n",
        "\n",
        "        for weights in self.layer_weights:\n",
        "            z = np.dot(activation, weights)\n",
        "            zs.append(z)\n",
        "            activation = self.sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        gradients = [None] * len(self.layer_weights)\n",
        "        error = activations[-1] - output_matrix\n",
        "\n",
        "        for l in range(1, len(self.layer_weights) + 1):\n",
        "            z = zs[-l]\n",
        "            activation_derivative = self.sigmoid_prime(z)\n",
        "            g_l = error * activation_derivative\n",
        "\n",
        "            grad_l = np.dot(activations[-l-1].T, g_l) / input_matrix.shape[0]\n",
        "            gradients[-l] = grad_l\n",
        "\n",
        "            if l < len(self.layer_weights):\n",
        "                error = np.dot(g_l, self.layer_weights[-l].T)\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def train(self,\n",
        "              input_matrix: np.ndarray,\n",
        "              output_matrix: np.ndarray,\n",
        "              iterations: int = 10,\n",
        "              learning_rate: float = 0.1) -> None:\n",
        "        \"\"\"Trains the neural network on an input matrix and an expected output\n",
        "        matrix.\n",
        "\n",
        "        Training should repeatedly (`iterations` times) calculate the gradients,\n",
        "        and update the model by subtracting the learning rate times the\n",
        "        gradients from the model weight matrices.\n",
        "\n",
        "        :param input_matrix: The matrix of inputs to the network, where each\n",
        "        row in the matrix represents an instance for which the neural network\n",
        "        should make a prediction\n",
        "        :param output_matrix: A matrix of expected outputs, where each row is\n",
        "        the expected outputs - each either 0 or 1 - for the corresponding row in\n",
        "        the input matrix.\n",
        "        :param iterations: The number of gradient descent steps to take.\n",
        "        :param learning_rate: The size of gradient descent steps to take, a\n",
        "        number that the gradients should be multiplied by before updating the\n",
        "        model weights.\n",
        "        \"\"\"\n",
        "        for _ in range(iterations):\n",
        "            grads = self.gradients(input_matrix, output_matrix)\n",
        "            self.layer_weights = [w - learning_rate * grad for w, grad in zip(self.layer_weights, grads)]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
